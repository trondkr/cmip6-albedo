{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "import os\n",
    "import seaborn as sns\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from shapely.geometry import box, mapping\n",
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "from matplotlib import cm\n",
    "import cartopy.feature as cpf\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import texttable\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from xclim import ensembles\n",
    "import global_land_mask\n",
    "import iris\n",
    "import logging\n",
    "import sys\n",
    "sys.path.append(\"../Farallon/QIN/CMIP6-downscale/\")\n",
    "sys.path.append(\"../Farallon/QIN/CMIP6-downscale/isimip3basd/\")\n",
    "from CMIP6_ridgeplot import CMIP6_ridgeplot\n",
    "import CMIP6_downscale_iris\n",
    "from CMIP6_model import CMIP6_MODEL\n",
    "from CMIP6_plot import CMIP6_GLORYS12_plot\n",
    "from cartopy.util import add_cyclic_point\n",
    "logging.getLogger('xclim').setLevel('ERROR')\n",
    "logging.getLogger('root').setLevel('ERROR')\n",
    "sns.set(font_scale=1.5, style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_180(ds):\n",
    " #   ds=ds.assign_coords(lat=ds.y)\n",
    "    return (ds.assign_coords(lon=(((ds.lon + 180) % 360) - 180))).sortby('lon')\n",
    "\n",
    "def convert_time(ds):\n",
    "    if not ds.indexes[\"time\"].dtype in [\"datetime64[ns]\"]:\n",
    "\n",
    "        time_objects = ds.indexes['time'].to_datetimeindex() \n",
    "        ds=ds.assign_coords({\"time\": time_objects})                   \n",
    "        ds = xr.decode_cf(ds)\n",
    "        \n",
    "    return ds\n",
    "\n",
    "def xr_add_cyclic_point(da):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    da: xr.DataArray with dimensions (time,lat,lon)\n",
    "    \"\"\"\n",
    "\n",
    "    # Use add_cyclic_point to interpolate input data\n",
    "    lon_idx = 2 #da.dims.index('lon')\n",
    "\n",
    "    wrap_data, wrap_lon = add_cyclic_point(da.values, coord=da.lon, axis=lon_idx)\n",
    "\n",
    "    # Generate output DataArray with new data but same structure as input\n",
    "    outp_da = xr.DataArray(data=wrap_data,\n",
    "                           coords = {'time': da.time, 'lat': da.lat, 'lon': wrap_lon},\n",
    "                           dims=da.dims,\n",
    "                           attrs=da.attrs)\n",
    "\n",
    "    return outp_da\n",
    "\n",
    "def create_land_ocean_mask(cmip6_grid: xr.Dataset) -> xr.DataArray:\n",
    "    print(\"[create_land_ocean_mask] Running create_land_ocean_mask\")\n",
    "    lon = cmip6_grid.lon.values\n",
    "    lat = cmip6_grid.lat.values\n",
    "    lon_180 = xr.where(lon > 180, lon - 360, lon)\n",
    "    lon_grid, lat_grid = np.meshgrid(lon_180, lat)\n",
    "\n",
    "    mask_data = global_land_mask.globe.is_ocean(lat_grid, lon_grid).astype(int)\n",
    "    #  mask_data[np.isnan(mask_data)] = np.nan\n",
    "\n",
    "    return xr.DataArray(mask_data, coords={'lat': lat, 'lon': lon},\n",
    "                        dims=['lat', 'lon'])\n",
    "\n",
    "def calculate_areacello(ds, model_obj, var_name, project_name):\n",
    "\n",
    "        # Calculate the area based on the longitude - latitude\n",
    "        if ds.lon.ndim == 2:\n",
    "            lon = ds.lon.values[0, :]\n",
    "            lat = ds.lat.values[:, 0]\n",
    "        else:\n",
    "            lon = ds.lon.values\n",
    "            lat = ds.lat.values\n",
    "\n",
    "        ds_singletime = ds.isel(time=0)\n",
    "\n",
    "        # Convert the dataset to a cube as this adds correct units required by iris\n",
    "        sdiris = CMIP6_downscale_iris.CMIP6_downscale_iris()\n",
    "        cube = sdiris.ds_to_iris(ds_singletime,\n",
    "                                 var_name,\n",
    "                                 model_obj,\n",
    "                                 project_name, prefix=\"cube\")\n",
    "\n",
    "        # Calculate the areacello for the grid and convert the result to km2\n",
    "        # Uses iris area_weights function.\n",
    "        # https://scitools.org.uk/iris/docs/v2.4.0/iris/iris/analysis/cartography.html#iris.analysis.cartography.area_weights\n",
    "        m2_to_km2 = 1.0e-6\n",
    "        area_ends = (iris.analysis.cartography.area_weights(cube, normalize=False)) * m2_to_km2\n",
    "\n",
    "        # Now convert the numpy array of areas to a dataset with the same dimension as the siconc\n",
    "        area_ds = xr.DataArray(name=\"areacello\",\n",
    "                               data=area_ends,\n",
    "                               coords={\"lat\": lat,\n",
    "                                       \"lon\": lon},\n",
    "                               dims=[\"lat\", \"lon\"]).to_dataset()\n",
    "\n",
    "        # Convert the resulting dataset to an iris cube\n",
    "        area_cube = sdiris.ds_to_iris(area_ds,\n",
    "                                      \"areacello\",\n",
    "                                      model_obj,\n",
    "                                      project_name,\n",
    "                                      prefix=\"areacello_\")\n",
    "\n",
    "        # Fix the coordinates so that we add geographic information to the cube,\n",
    "        # before saving the cube to the siconc dataset\n",
    "        area_cube = sdiris.fix_coordinates_cube(area_cube)\n",
    "\n",
    "        return xr.DataArray.from_iris(area_cube)\n",
    "\n",
    "\n",
    "def get_area_averaged_ds(fname, model, scenario, ensemble_id, var_name, LME, create_maps, frequency, models_dict):\n",
    "    \n",
    "    if os.path.exists(fname):\n",
    "        with xr.open_dataset(fname) as ds:\n",
    "            ds = convert_to_180(ds)\n",
    "            ds = ds.sel(time=slice(start_time,end_time))\n",
    "\n",
    "            ds = convert_time(ds)\n",
    "            ds_lme = get_data_within_LME(ds, var_name, LME, False)\n",
    "            ds_lme = xr_add_cyclic_point(ds_lme[var_name])\n",
    "            ds_lme = ds_lme.to_dataset(name=var_name)\n",
    "\n",
    "            # Add land mask\n",
    "            ds_lme[\"mask\"] = create_land_ocean_mask(ds_lme)\n",
    "            ds_lme = ds_lme.where(ds_lme.mask == 1)\n",
    "            model_obj = CMIP6_MODEL(model)\n",
    "\n",
    "            ds_lme[\"areacello\"] = calculate_areacello(ds_lme, model_obj, var_name, project_name=\"light\")\n",
    "            # Clip the area to the polygon - this also clips to the time varying var_name# which results in\n",
    "            # time varying areacello variable that we can use to sum up the size of open water by\n",
    "            # looking at the annual mean.\n",
    "            # First we set all areas outside of the polygon to nan - which also now includes\n",
    "            # areacello which as part of teh calculations actually covered the whole region (and\n",
    "            # not just the polygon).\n",
    "            ds_lme = xr.where(np.isnan(ds_lme[var_name]), np.nan, ds_lme)\n",
    "\n",
    "            # Then we remove the areas where par or uv is less than a minimum value to remove zeroes etc.\n",
    "            if var_name in [\"par\"]:\n",
    "                ds_lme = xr.where(ds_lme[var_name]<1, np.nan, ds_lme)\n",
    "\n",
    "            elif var_name in [\"uvb\"]:\n",
    "                ds_lme = xr.where(ds_lme[var_name]<0.001, np.nan, ds_lme)\n",
    "            total_area_lme = np.nansum(ds_lme[\"areacello\"].mean(dim=\"time\").values)\n",
    "            print(\"[calculate_areacello] Average total area {:,.2f} km2\".format(total_area_lme))\n",
    "\n",
    "\n",
    "            if LME in [\"Barents Sea\",\"Arctic Ocean\"]:\n",
    "                projection=ccrs.NorthPolarStereo() #ccrs.PlateCarree(central_longitude=0)\n",
    "                extent = [-10, 80, 67, 85]\n",
    "            else:\n",
    "                projection=ccrs.PlateCarree(central_longitude=-180)\n",
    "                extent = [-184, -155, 60, 80]\n",
    "                extent = [-200, -145, 50, 85]\n",
    "\n",
    "            if create_maps:\n",
    "                fig = plt.figure(figsize=(13, 8))\n",
    "                ax1 = fig.add_subplot(111, projection=projection)\n",
    "                create_LME_figure(ax1, [LME], ccrs.PlateCarree(central_longitude=-180),\n",
    "                                  True,extent,ds_lme[\"areacello\"].isel(time=-1))\n",
    "                plt.show()\n",
    "            # Average data within the LME as a function of time\n",
    "            ds = ds_lme.mean({\"lat\",\"lon\"})\n",
    "\n",
    "            # To get the average sum of the open water areas we sum prio to creating the\n",
    "            # dataframe. Then when running rolling mean we just get the summed value back smoothed.\n",
    "            ds = ds.assign(areacello = ds_lme[\"areacello\"].sum({\"lat\",\"lon\"}))\n",
    "\n",
    "            df = ds.to_dataframe().dropna()\n",
    "            df = df.reset_index()\n",
    "\n",
    "        roll_years=5\n",
    "        df=df.resample(frequency, on=\"time\").mean()\n",
    "\n",
    "        df[\"model_name\"]=model\n",
    "        df[\"roll_mean\"]=df[var_name].rolling(roll_years).mean()\n",
    "        df[\"roll_mean_area\"]=df[\"areacello\"].rolling(roll_years).mean()\n",
    "        df[\"roll_median\"]=df[var_name].rolling(roll_years).median()\n",
    "        df[\"roll_max\"]=df[var_name].rolling(roll_years).max()\n",
    "        df[\"roll_min\"]=df[var_name].rolling(roll_years).min()\n",
    "\n",
    "        df[\"model_ensemble_id\"]=ensemble_id\n",
    "        df[\"model_scenario\"]=scenario\n",
    "        df[\"model_name_scenario\"]=model+\"_\"+scenario\n",
    "        unique=\"{}_{}_{}\".format(model, scenario, ensemble_id)\n",
    "        df[\"unique\"]=unique\n",
    "\n",
    "        model_info={}\n",
    "        model_info[\"model_name\"]=model\n",
    "        model_info[\"model_scenario\"]=scenario\n",
    "        model_info[\"model_ensemble_id\"]=ensemble_id\n",
    "        model_info[\"model_var\"]=var_name\n",
    "        key=\"{}_{}_{}_{}\".format(model,ensemble_id,scenario,var_name)\n",
    "        formatter=\"{:.2f}\"\n",
    "        model_info[\"model_min\"]=formatter.format(np.nanmin(df[var_name]))\n",
    "        model_info[\"model_max\"]=formatter.format(np.nanmax(df[var_name]))\n",
    "\n",
    "        models_dict[key]=model_info\n",
    "        return df, models_dict, ds_lme, total_area_lme, extent, projection\n",
    "    else:\n",
    "        return None, models_dict, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_LME_records():\n",
    "    lme_file='../oceanography/Shapefiles/LME66/LMEs66.shp'\n",
    "    return gpd.read_file(lme_file)\n",
    "\n",
    "def get_LME_records_plot():\n",
    "    lme_file='../oceanography/Shapefiles/LME66_180/LME66_180.shp'\n",
    "    return gpd.read_file(lme_file)\n",
    "\n",
    "def create_colors(N):\n",
    "    color=iter(cm.tab20b(np.linspace(0,1,N)))\n",
    "    return [next(color) for c in range(N)]\n",
    "\n",
    "\n",
    "def create_LME_figure(ax, LMES, projection, show, extent,data_to_contour=None):\n",
    "\n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    \n",
    "    ax.set_extent(extent)\n",
    "\n",
    "    # Get the -180-180 projected shapefile containing LMEs to make it\n",
    "    # easy to plot across the Pacific Ocean\n",
    "    shdf = get_LME_records_plot()\n",
    "    colors_rgb=create_colors(len(LMES))\n",
    "    counter=0\n",
    "    for LME_NAME,LME_NUMBER in zip(shdf['LME_NAME'],shdf['LME_NUMBER']):\n",
    "\n",
    "        shdf_sel = shdf[ shdf['LME_NAME']==LME_NAME ]\n",
    "\n",
    "        if (LME_NAME in LMES):\n",
    "           # print(\"Adding geometry for LME {}\".format(LME_NAME))\n",
    "            # Add the geometry and fill it with color\n",
    "            if len(LMES)==1:\n",
    "                color=\"red\"\n",
    "            else:\n",
    "                color=colors_rgb[counter]\n",
    "            ax.add_geometries(shdf_sel['geometry'],\n",
    "                              projection,\n",
    "                              facecolor=color,\n",
    "                              edgecolor='k',\n",
    "                              zorder=8)\n",
    "            if data_to_contour is not None:\n",
    "\n",
    "                ax.contourf(data_to_contour.lon,\n",
    "                            data_to_contour.lat,\n",
    "                            data_to_contour,\n",
    "                            zorder=10,\n",
    "                   cmap=sns.color_palette(\"Spectral_r\", as_cmap=True),\n",
    "                   transform=ccrs.PlateCarree())\n",
    "\n",
    "            # Add the label LME_NUMBER of the selected LME at the center of the LME\n",
    "          #  ax.annotate(s=LME_NUMBER,\n",
    "          #              xy=(shdf_sel.centroid.x,shdf_sel.centroid.y),\n",
    "          #              color=\"white\",\n",
    "          #              fontsize=13)\n",
    "            counter+=1\n",
    "        else:\n",
    "            ax.add_geometries(shdf_sel['geometry'],\n",
    "                              projection,\n",
    "                              facecolor='LightGray',\n",
    "                              edgecolor='k')\n",
    "\n",
    "    if show:\n",
    "        plotfile=\"Figures/CMIP6_lightpaper_map_{}.png\".format(LMES[0])\n",
    "        print(\"Created figure {}\".format(plotfile))\n",
    "        plt.savefig(plotfile, dpi=200,\n",
    "                        bbox_inches = 'tight')\n",
    "        plt.show()\n",
    "\n",
    "def get_data_within_LME(ds,var_name,LME,create_maps):\n",
    "\n",
    "    print(\"Working on LME: {}\".format(LME))\n",
    "    \n",
    "    # Extract the polygon defining the boundaries of the LME\n",
    "    shdf = get_LME_records()\n",
    "   # for name in shdf['LME_NAME']:\n",
    "   #     print(name)\n",
    "    shdf_sel = shdf[ shdf['LME_NAME']==LME ]\n",
    "\n",
    "    # Create the map of the LME boundaries and color it.\n",
    "    # The active LME has color while the others are grey.\n",
    "    if create_maps:\n",
    "        # Setup the figure panels\n",
    "        fig = plt.figure(figsize=(13, 8))\n",
    "        if LME in [\"Barents Sea\",\"Arctic Ocean\"]:\n",
    "            projection=ccrs.NorthPolarStereo() #ccrs.PlateCarree(central_longitude=0)\n",
    "            extent = [-8, 80, 67, 90]\n",
    "        else:\n",
    "            projection=ccrs.PlateCarree(central_longitude=-180)\n",
    "            extent = [-252, -100, 10, 65]\n",
    "            extent = [-220, -135, 30, 85]\n",
    "        ax1 = fig.add_subplot(111, projection=projection)\n",
    "    \n",
    "        create_LME_figure(ax1, [LME], ccrs.PlateCarree(central_longitude=-180),True,extent)\n",
    "\n",
    "    # Rioxarray requires x and y dimensions - we convert these back to lon and lat later.\n",
    "    # We also add the projection (lat-lon) so that rioxarray can do the clipping of the data according to the\n",
    "    # shapefile.\n",
    "\n",
    "    tos=ds.rename({'lon': 'x','lat': 'y'})\n",
    "    tos=tos.rio.write_crs(4326)\n",
    "\n",
    "    # Clip the data within the LME. We have to convert the polygon geometry to a geodataframe using\n",
    "    # `shapely.geometry`. The clipping of data within the polygon is done using rioxarray.clip function\n",
    "\n",
    "    clipped = tos.rio.clip(geometries=shdf_sel.geometry.apply(mapping), crs=tos.rio.crs)\n",
    "    clipped=clipped.rename({'x': 'lon','y': 'lat'}) #.to_dataset()\n",
    "\n",
    "    p1=\"2000-01-01 to 2020-01-01\"\n",
    "    p2=\"2080-01-01 to 2020-01-01\"\n",
    "\n",
    "    create_maps=False\n",
    "    if create_maps:\n",
    "        clipped_p1=clipped.sel(time=slice(\"2000-01-01\",\"2020-01-01\")).mean({\"time\"})\n",
    "       # clipped_p2=clipped.sel(time=slice(\"2080-01-01\",\"2099-12-16\")).mean({\"time\"})\n",
    "\n",
    "        create_map(clipped_p1, \"{} 2000-01-01 to 2020-01-01\".format(var_name), var_name, period=p1, anomalies=False, details=False)\n",
    "        #create_map(clipped_p2, \"{} 2080-01-01 to 2020-01-01\".format(var_name), var_name, period=p2, anomalies=False, details=False)\n",
    "\n",
    "        plt.show()\n",
    "    return clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_summary_table(dict_of_models, LME):\n",
    "    table = texttable.Texttable()\n",
    "    table.set_cols_align([\"c\",\"c\", \"c\",\"c\",\"c\",\"c\",\"c\"])\n",
    "    table.set_cols_valign([\"t\",\"t\", \"m\",\"m\",\"m\",\"m\", \"b\"])\n",
    "\n",
    "    table.header([\"LME\",\"Model\", \"Scenario\", \"ID\", \"Var\", \"CMIP6 min\", \"CMIP6 max\"])\n",
    "    for key in dict_of_models.keys():\n",
    "        model=dict_of_models[key]\n",
    "\n",
    "        table.add_row([LME,\n",
    "                       model[\"model_name\"],\n",
    "                       model[\"model_scenario\"],\n",
    "                       model[\"model_ensemble_id\"],\n",
    "                       str(model[\"model_var\"]),\n",
    "                       str(model[\"model_min\"]),\n",
    "                       str(model[\"model_max\"])])\n",
    "\n",
    "    table.set_cols_width([30,30,20,20,10,10,10])\n",
    "    print(table.draw() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on LME: Barents Sea\n",
      "[create_land_ocean_mask] Running create_land_ocean_mask\n",
      "[calculate_areacello] Average total area 2,014,185.04 km2\n",
      "Created dataframe of file: ../oceanography/light/ncfiles/par_ACCESS-ESM1-5_r1i1p1f1_1979-01-01-2099-12-16_scenario_osa_ssp245.nc\n",
      "Working on LME: Barents Sea\n",
      "[create_land_ocean_mask] Running create_land_ocean_mask\n",
      "[calculate_areacello] Average total area 2,014,185.04 km2\n",
      "Created dataframe of file: ../oceanography/light/ncfiles/par_MPI-ESM1-2-LR_r2i1p1f1_1979-01-01-2099-12-16_scenario_osa_ssp245.nc\n",
      "Working on LME: Barents Sea\n",
      "[create_land_ocean_mask] Running create_land_ocean_mask\n",
      "[calculate_areacello] Average total area 2,014,185.04 km2\n",
      "Created dataframe of file: ../oceanography/light/ncfiles/par_UKESM1-0-LL_r1i1p1f2_1979-01-01-2099-12-16_scenario_osa_ssp245.nc\n",
      "Working on LME: Barents Sea\n",
      "[create_land_ocean_mask] Running create_land_ocean_mask\n",
      "[calculate_areacello] Average total area 2,014,185.04 km2\n",
      "Created dataframe of file: ../oceanography/light/ncfiles/par_ACCESS-ESM1-5_r1i1p1f1_1979-01-01-2099-12-16_scenario_osa_ssp585.nc\n",
      "Working on LME: Barents Sea\n",
      "[create_land_ocean_mask] Running create_land_ocean_mask\n",
      "[calculate_areacello] Average total area 2,014,185.04 km2\n",
      "Created dataframe of file: ../oceanography/light/ncfiles/par_MPI-ESM1-2-LR_r2i1p1f1_1979-01-01-2099-12-16_scenario_osa_ssp585.nc\n",
      "Working on LME: Barents Sea\n",
      "[create_land_ocean_mask] Running create_land_ocean_mask\n",
      "[calculate_areacello] Average total area 2,014,185.04 km2\n",
      "Created dataframe of file: ../oceanography/light/ncfiles/par_UKESM1-0-LL_r1i1p1f2_1979-01-01-2099-12-16_scenario_osa_ssp585.nc\n",
      "+--------------------------------+--------------------------------+----------------------+----------------------+------------+------------+------------+\n",
      "|              LME               |             Model              |       Scenario       |          ID          |    Var     | CMIP6 min  | CMIP6 max  |\n",
      "+================================+================================+======================+======================+============+============+============+\n",
      "|          Barents Sea           |         ACCESS-ESM1-5          |        ssp245        |       r1i1p1f1       |    par     |   41.560   |   50.920   |\n",
      "+--------------------------------+--------------------------------+----------------------+----------------------+------------+------------+------------+\n",
      "|          Barents Sea           |         MPI-ESM1-2-LR          |        ssp245        |       r2i1p1f1       |    par     |   40.470   |   50.230   |\n",
      "+--------------------------------+--------------------------------+----------------------+----------------------+------------+------------+------------+\n",
      "|          Barents Sea           |          UKESM1-0-LL           |        ssp245        |       r1i1p1f2       |    par     |   30.720   |   52.150   |\n",
      "+--------------------------------+--------------------------------+----------------------+----------------------+------------+------------+------------+\n",
      "|          Barents Sea           |         ACCESS-ESM1-5          |        ssp585        |       r1i1p1f1       |    par     |   41.220   |   51.630   |\n",
      "+--------------------------------+--------------------------------+----------------------+----------------------+------------+------------+------------+\n",
      "|          Barents Sea           |         MPI-ESM1-2-LR          |        ssp585        |       r2i1p1f1       |    par     |   40.470   |   51.240   |\n",
      "+--------------------------------+--------------------------------+----------------------+----------------------+------------+------------+------------+\n",
      "|          Barents Sea           |          UKESM1-0-LL           |        ssp585        |       r1i1p1f2       |    par     |   30.720   |   53.220   |\n",
      "+--------------------------------+--------------------------------+----------------------+----------------------+------------+------------+------------+\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [131]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    115\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mgroupby(level\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m var_name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpar\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muvb\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muv\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 117\u001B[0m     df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mroll_mean\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mroll_mean\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnan\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28mprint\u001B[39m(df\u001B[38;5;241m.\u001B[39mdescribe())\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mprint\u001B[39m(df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mroll_mean\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mloc[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1980-01-01\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m2000-01-01\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/light/lib/python3.9/site-packages/pandas/core/groupby/generic.py:223\u001B[0m, in \u001B[0;36mSeriesGroupBy.apply\u001B[0;34m(self, func, *args, **kwargs)\u001B[0m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;129m@Appender\u001B[39m(\n\u001B[1;32m    218\u001B[0m     _apply_docs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemplate\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    219\u001B[0m         \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseries\u001B[39m\u001B[38;5;124m\"\u001B[39m, examples\u001B[38;5;241m=\u001B[39m_apply_docs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseries_examples\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    220\u001B[0m     )\n\u001B[1;32m    221\u001B[0m )\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 223\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/light/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:1275\u001B[0m, in \u001B[0;36mGroupBy.apply\u001B[0;34m(self, func, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1273\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode.chained_assignment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   1274\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1275\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_python_apply_general\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selected_obj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1276\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   1277\u001B[0m         \u001B[38;5;66;03m# gh-20949\u001B[39;00m\n\u001B[1;32m   1278\u001B[0m         \u001B[38;5;66;03m# try again, with .apply acting as a filtering\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1282\u001B[0m         \u001B[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001B[39;00m\n\u001B[1;32m   1283\u001B[0m         \u001B[38;5;66;03m# on a string grouper column\u001B[39;00m\n\u001B[1;32m   1285\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m group_selection_context(\u001B[38;5;28mself\u001B[39m):\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/light/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:1309\u001B[0m, in \u001B[0;36mGroupBy._python_apply_general\u001B[0;34m(self, f, data)\u001B[0m\n\u001B[1;32m   1290\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[1;32m   1291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_python_apply_general\u001B[39m(\n\u001B[1;32m   1292\u001B[0m     \u001B[38;5;28mself\u001B[39m, f: F, data: FrameOrSeriesUnion\n\u001B[1;32m   1293\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m FrameOrSeriesUnion:\n\u001B[1;32m   1294\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1295\u001B[0m \u001B[38;5;124;03m    Apply function f in python space\u001B[39;00m\n\u001B[1;32m   1296\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;124;03m        data after applying f\u001B[39;00m\n\u001B[1;32m   1308\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1309\u001B[0m     keys, values, mutated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrouper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1311\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_applied_output(\n\u001B[1;32m   1312\u001B[0m         data, keys, values, not_indexed_same\u001B[38;5;241m=\u001B[39mmutated \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmutated\n\u001B[1;32m   1313\u001B[0m     )\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/light/lib/python3.9/site-packages/pandas/core/groupby/ops.py:852\u001B[0m, in \u001B[0;36mBaseGrouper.apply\u001B[0;34m(self, f, data, axis)\u001B[0m\n\u001B[1;32m    850\u001B[0m \u001B[38;5;66;03m# group might be modified\u001B[39;00m\n\u001B[1;32m    851\u001B[0m group_axes \u001B[38;5;241m=\u001B[39m group\u001B[38;5;241m.\u001B[39maxes\n\u001B[0;32m--> 852\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_indexed_like(res, group_axes, axis):\n\u001B[1;32m    854\u001B[0m     mutated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "Input \u001B[0;32mIn [131]\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    115\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mgroupby(level\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m var_name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpar\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muvb\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muv\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 117\u001B[0m     df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mroll_mean\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mroll_mean\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x : x \u001B[38;5;28;01mif\u001B[39;00m x \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m np\u001B[38;5;241m.\u001B[39mnan)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28mprint\u001B[39m(df\u001B[38;5;241m.\u001B[39mdescribe())\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mprint\u001B[39m(df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mroll_mean\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mloc[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1980-01-01\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m2000-01-01\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/light/lib/python3.9/site-packages/pandas/core/generic.py:1537\u001B[0m, in \u001B[0;36mNDFrame.__nonzero__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1535\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__nonzero__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m-> 1537\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1538\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe truth value of a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is ambiguous. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1540\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "scenarios = [\"ssp245\", \"ssp585\"]\n",
    "member_range = 12\n",
    "frequency = \"A\"\n",
    "ensemble_ids = [\"r{}i{}p{}f{}\".format(str(i + 1), str(ii + 1), str(iii + 1), str(iv + 1)) for i in\n",
    "                range(member_range)\n",
    "                for ii in range(member_range) for iii in range(member_range) for iv in range(member_range)]\n",
    "period = \"1979-01-01-2099-12-16\"\n",
    "start_time = \"1979-01-01\"\n",
    "end_time = \"2099-12-16\"\n",
    "\n",
    "models = [\"ACCESS-ESM1-5\", \"MPI-ESM1-2-LR\", \"CMCC-ESM2\", \"UKESM1-0-LL\"]\n",
    "# \"CanESM5-CanOE\",\"UKESM1-0-LL\",\"MPI-ESM1-2-HR\"]\n",
    "\n",
    "#models=[\"CMCC-ESM2\"]\n",
    "\n",
    "ds_var_names = [\"par\"] #, \"uvb\", \"uv\"]\n",
    "#ds_var_names = [\"uv\",\"uvb\"]  #,\"uv\"]\n",
    "write_stats_to_file = False\n",
    "\n",
    "LMES = ['California Current', 'East Bering Sea', 'Gulf of Alaska',\n",
    "        'Northern Bering - Chukchi Seas', 'West Bering Sea', 'Sea of Japan',\n",
    "        'Oyashio Current', 'Kuroshio Current', 'East China Sea',\n",
    "        'South China Sea', 'Sea of Okhotsk', 'Yellow Sea',\n",
    "        'Aleutian Islands']\n",
    "\n",
    "LMES = ['Barents Sea', 'Northern Bering - Chukchi Seas', 'Central Arctic']\n",
    "LMES = ['Barents Sea', 'Northern Bering - Chukchi Seas'] #, 'Central Arctic', 'East Bering Sea']\n",
    "\n",
    "for var_name in ds_var_names:\n",
    "    for LME in LMES:\n",
    "        df_list = []\n",
    "        models_dict = {}\n",
    "        simulation = \"osa\"\n",
    "        create_maps = False\n",
    "        saved_total_area_lme = None\n",
    "        saved_extent = None\n",
    "        saved_projection = None\n",
    "        # We loop over all of the scenarios, ensemble_ids, and models to create a\n",
    "        # list of dataframes that we eventually concatenate together and plot\n",
    "        for scenario in scenarios:\n",
    "            ds_list = []\n",
    "\n",
    "            for model in models:\n",
    "                for ensemble_id in ensemble_ids:\n",
    "\n",
    "                    fname = \"../oceanography/light/ncfiles/{}_{}_{}_{}_scenario_{}_{}.nc\".format(var_name,\n",
    "                                                                                                 model,\n",
    "                                                                                                 ensemble_id,\n",
    "                                                                                                 period,\n",
    "                                                                                                 simulation,\n",
    "                                                                                                 scenario)\n",
    "\n",
    "                    df, models_dict, ds_lme, total_area_lme, extent, projection = get_area_averaged_ds(fname, model,\n",
    "                                                                                                       scenario,\n",
    "                                                                                                       ensemble_id,\n",
    "                                                                                                       var_name, LME,\n",
    "                                                                                                       create_maps,\n",
    "                                                                                                       frequency,\n",
    "                                                                                                       models_dict)\n",
    "\n",
    "                    if ds_lme is not None:\n",
    "                        saved_extent = extent\n",
    "                        saved_projection = projection\n",
    "                        saved_total_area_lme = total_area_lme\n",
    "                        ds_lme = xr.where(((ds_lme < 1.e-20) | (ds_lme > 1e20)), np.nan, ds_lme)\n",
    "                        ds_list.append(ds_lme)\n",
    "                    create_maps = False\n",
    "                    if df is not None:\n",
    "                        df_list.append(df)\n",
    "                        print(\"Created dataframe of file: {}\".format(fname))\n",
    "\n",
    "            if len(ds_list) > 0:\n",
    "                ens = ensembles.create_ensemble(ds_list).load()\n",
    "                ens.close()\n",
    "                ens_stats = ensembles.ensemble_mean_std_max_min(ens)\n",
    "\n",
    "                outfile = \"Figures/{}_ensemble_{}_{}.png\".format(var_name.capitalize(), scenario, LME)\n",
    "\n",
    "                plot_clim=False\n",
    "                if plot_clim:\n",
    "                    for clim_start_time, clim_end_time in zip([\"2050-01-01\", \"2080-01-01\"],\n",
    "                                                              [\"2060-01-01\", \"2090-01-01\"]):\n",
    "                        print(\"[CMIP6_plot] Creating climatology plot for {} to {}\".format(start_time, end_time))\n",
    "\n",
    "                        cmip6 = CMIP6_GLORYS12_plot()\n",
    "                       # ens_stats = xr.where(ens_stats==0, 0, 1)\n",
    "                        \"\"\"\n",
    "                        cmip6.plot_monthly_climatology(\"{}_mean\".format(var_name),\n",
    "                                                       \"surface\", \"20\",\n",
    "                                                       ens_stats,\n",
    "                                                       \"Figures/\",\n",
    "                                                       prefix=\"light\",\n",
    "                                                       start_time=clim_start_time,\n",
    "                                                       end_time=clim_end_time,\n",
    "                                                       delta=True,\n",
    "                                                       extent=saved_extent,\n",
    "                                                       projection=saved_projection)\n",
    "\"\"\"\n",
    "                    CMIP6_ridgeplot.ridgeplot(\"{}_mean\".format(var_name),\n",
    "                                              None, outfile,\n",
    "                                                      glorys=False, depth_threshold=None,\n",
    "                                                      ds=ens_stats)\n",
    "\n",
    "        if len(df_list) > 0:\n",
    "            df = pd.concat(df_list)\n",
    "            df = df.reindex()\n",
    "            df[\"date\"]=df.index\n",
    "\n",
    "            create_summary_table(models_dict, LME)\n",
    "            df['time'] = pd.to_datetime(df.index, errors='coerce', format='%Y-%m-%d')\n",
    "            if os.path.exists(\"test.csv\"):os.remove(\"test.csv\")\n",
    "            df.to_csv(\"test.csv\")\n",
    "            df = pd.read_csv('test.csv', parse_dates=['time', 'date'])\n",
    "            df = df.set_index(\"time\")\n",
    "            df = df.groupby(level='time')\n",
    "            if var_name in [\"par\",\"uvb\", \"uv\"]:\n",
    "                df[\"roll_mean\"] = df[\"roll_mean\"].apply(lambda x : x if x > 0 else np.nan)\n",
    "            print(df.describe())\n",
    "            print(df[\"roll_mean\"].loc['1980-01-01':'2000-01-01'])\n",
    "            clim = (df[\"roll_mean\"].loc['1980-01-01':'2000-01-01']).mean()\n",
    "\n",
    "            df[\"rel_change\"] = ((df[\"roll_mean\"] - float(clim)) / float(clim)) * 100.\n",
    "\n",
    "            sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)}, font_scale=1.5)\n",
    "            f = plt.figure(figsize=(10, 10))\n",
    "            gs = f.add_gridspec(1, 1)\n",
    "            ax = f.add_subplot(gs[0, 0])\n",
    "\n",
    "            sns.set_palette(\"tab10\")\n",
    "\n",
    "            b = sns.lineplot(ax=ax, data=df, x=df.index, y=df[\"roll_mean\"],\n",
    "                             hue=df[\"model_scenario\"],\n",
    "                             alpha=.0, linewidth=0)\n",
    "\n",
    "            b.tick_params(labelsize=22)\n",
    "            b.set_xlabel(\"\", fontsize=20)\n",
    "            b.set_ylabel(\"\", fontsize=20)\n",
    "\n",
    "            ax2 = ax.twinx()\n",
    "\n",
    "            b2 = sns.lineplot(ax=ax2, data=df, x=df.index, y=df[\"rel_change\"],\n",
    "                           #   hue=df[\"model_scenario\"],\n",
    "                              alpha=.95, ci=95, linewidth=5, label=None)\n",
    "\n",
    "            b2.tick_params(labelsize=22)\n",
    "            b2.set_xlabel(\"\", fontsize=20)\n",
    "            b2.set_ylabel(\"\", fontsize=20)\n",
    "            ylabels = ['{:,.0%}'.format(y) for y in b2.get_yticks() / 100.]\n",
    "            b2.set_yticklabels(ylabels)\n",
    "            import matplotlib.dates as mdates\n",
    "            ax.xaxis.set_major_locator(mdates.YearLocator(base=10))\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "            plt.setp(ax.xaxis.get_majorticklabels(), rotation=-90)\n",
    "            b2.get_legend().remove()\n",
    "            ax.legend(loc=\"upper left\", frameon=False, fontsize=32)\n",
    "\n",
    "            if not os.path.exists(\"Figures\"):\n",
    "                os.makedirs(\"Figures\")\n",
    "            plotfile = \"Figures/CMIP6_light_{}_{}.png\".format(var_name, LME)\n",
    "            print(\"Created figure {}\".format(plotfile))\n",
    "            plt.savefig(plotfile, dpi=200,\n",
    "                        bbox_inches='tight')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            # Plot individual figure the open water area\n",
    "            if var_name in [\"par\"]:\n",
    "                f2 = plt.figure(figsize=(16, 16))\n",
    "                gs2 = f2.add_gridspec(1, 1)\n",
    "                ax2 = f2.add_subplot(gs2[0, 0])\n",
    "\n",
    "                # Colors from deep default seaborn palette found here:\n",
    "                # https://github.com/mwaskom/seaborn/blob/master/seaborn/palettes.py\n",
    "                palette = sns.color_palette([\"#8172B3\", \"#64B5CD\"])\n",
    "                sns.set_palette(palette)\n",
    "\n",
    "                # Add extra variable - percentage change in size of open water area.\n",
    "                df[\"change_open_water\"] = (df[\"roll_mean_area\"] / float(saved_total_area_lme)) * 100.\n",
    "\n",
    "                # Plot the results\n",
    "                # Note that we do not actually plot the roll_mean_area in axis ax2 only in ax3 below\n",
    "\n",
    "                b11 = sns.lineplot(ax=ax2, data=df, x=df[\"date\"], y=df[\"roll_mean_area\"],\n",
    "                                 hue=df[\"model_scenario\"],\n",
    "                                 alpha=.0, ci=95, linewidth=0)\n",
    "\n",
    "                b11.tick_params(labelsize=22)\n",
    "                b11.set_xlabel(\"\", fontsize=20)\n",
    "                b11.set_ylabel(\"\", fontsize=20)\n",
    "\n",
    "                # Format the yticks\n",
    "                # https://mkaz.blog/code/python-string-format-cookbook/\n",
    "                ylabels = ['{:,}'.format(y) for y in b11.get_yticks()]\n",
    "                b11.set_yticklabels(ylabels)\n",
    "\n",
    "                print(\"Open water calculations range from {} on {} to {} in {}\".format(df[\"change_open_water\"].iloc[0],\n",
    "                                                                                       df[\"time\"].iloc[0],\n",
    "                                                                                       df[\"change_open_water\"].iloc[-1],\n",
    "                                                                                       df[\"time\"].iloc[-1]))\n",
    "                ax3 = ax2.twinx()\n",
    "                b22 = sns.lineplot(ax=ax3, data=df, x=df[\"date\"], y=df[\"change_open_water\"],\n",
    "                                  hue=df[\"model_scenario\"],\n",
    "                                  alpha=.95, ci=95, linewidth=5, label=None)\n",
    "\n",
    "                b22.tick_params(labelsize=22)\n",
    "                b22.set_xlabel(\"\", fontsize=20)\n",
    "                b22.set_ylabel(\"\", fontsize=20)\n",
    "                ylabels = ['{:,.0%}'.format(y) for y in b22.get_yticks() / 100.]\n",
    "                b22.set_yticklabels(ylabels)\n",
    "\n",
    "                import matplotlib.dates as mdates\n",
    "                ax2.xaxis.set_major_locator(mdates.YearLocator(base=10))\n",
    "                ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "                plt.setp(ax2.xaxis.get_majorticklabels(), rotation=-90)\n",
    "\n",
    "                b22.get_legend().remove()\n",
    "                ax2.legend(loc=\"upper left\", frameon=False, fontsize=32)\n",
    "\n",
    "                plotfile = \"Figures/CMIP6_light_{}_{}.png\".format(\"area_open_water\", LME)\n",
    "                print(\"Created figure {}\".format(plotfile))\n",
    "                plt.savefig(plotfile, dpi=300,\n",
    "                            bbox_inches='tight')\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        ds_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}